{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFHeTomzBOSjH//gIZvxYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wenlu057/RL-for-Traffic-Signal-Control/blob/main/Traffic_Signal_Control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprint**: A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control\n",
        "\n",
        "- Constrained Reinforcement Learning (CRL)-based method\n",
        "- A <u>constrained</u> multi-agent reinforcement learning (MARL) problem.\n",
        "- Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-\n",
        "LCE)\n",
        "- Our approach integrates the Lagrange multipliers method to balance rewards and constraints, with a cost estimator for stable adjustment.\n",
        "  - This means the method includes a **mathematical tool** (Lagrange multipliers) that's commonly used to **solve optimization problems** with **constraints**.\n",
        "  - Lagrange multipliers help by turning the constrained optimization into an unconstrained one, where penalties are added proportionally when constraints are violated.\n",
        "  - The system uses a cost estimator (likely a learned model or critic) to predict how much the constraints are being violated or how costly a given action is.\n",
        "\n",
        "- three constraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip"
      ],
      "metadata": {
        "id": "eHWbeq5Cl48a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Issue:**\n",
        "1. uncertainties about their **deployment** in real-world environments.\n",
        "    - Multi-Objective: Efficiency and Safety\n",
        "    - Fairness of each intersection. The green times for different lanes are the same on average.\n",
        "    - Namely, how to incorporat constraints into ATSC methods that accurately reflect the demands of real-world environments"
      ],
      "metadata": {
        "id": "8vyV5sKdvIsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When Do You Say a Method Is Heuristic?**\n",
        "\n",
        "A **heuristic** method is one that aims to find a **good enough solution** efficiently when:\n",
        "\n",
        "- Finding an exact solution is too costly (in terms of time, memory, or computation)\n",
        "- The problem is **NP-hard** or computationally intractable\n",
        "- An approximate or suboptimal solution is acceptable\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ Characteristics of Heuristic Methods**\n",
        "\n",
        "- **Rule of thumb**: Based on practical experience or intuition rather than formal proof\n",
        "- **No guarantee of optimality**: May not find the best solution\n",
        "- **Problem-specific**: Often tailored to the structure of a specific problem\n",
        "- **Efficient**: Trades off accuracy for speed or scalability\n",
        "\n",
        "---\n",
        "\n",
        "**üîç Examples of Heuristic Methods**\n",
        "\n",
        "- **Greedy Algorithms**: Make the locally optimal choice at each step  \n",
        "  _Example: Greedy knapsack, activity selection_\n",
        "  \n",
        "- **A\\* Search Algorithm**: Uses heuristics to estimate cost and guide search in graphs  \n",
        "  _Example: Pathfinding in maps and games_\n",
        "\n",
        "- **Simulated Annealing**: Randomly explores solution space with decreasing randomness\n",
        "\n",
        "- **Genetic Algorithms**: Inspired by natural selection, evolves a population of solutions\n",
        "\n",
        "---\n",
        "\n",
        "**üß† When to Call a Method Heuristic**\n",
        "\n",
        "Use the term **heuristic** when:\n",
        "\n",
        "- The approach relies on **informed guesses** or practical shortcuts\n",
        "- There is **no guarantee** it finds the optimal solution\n",
        "- It performs **well in practice** despite lacking formal guarantees\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Ve3ZsyJvyf2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Related Work**\n",
        " - Non-RL: Previous works on ATSC use the observations of the intersections to form traffic control policies,\n",
        "such as [SOTL](https://arxiv.org/pdf/nlin/0610040). ‚¨Ö heuristic-based\n",
        "\n",
        " - RL:\n",
        "   1. [FRAP-CIKM-2019](https://dl.acm.org/doi/pdf/10.1145/3357384.3357900)\n",
        "   2. [PressLight-KDD-2019](https://dl.acm.org/doi/pdf/10.1145/3292500.3330949)\n",
        "   3. [CoLight-CIKM-2019](https://dl.acm.org/doi/abs/10.1145/3357384.3357902) ‚¨Ö Multi-intersection\n",
        "   4.[AttendLight-NIPS-2020](https://proceedings.neurips.cc/paper/2020/file/29e48b79ae6fc68e9b6480b677453586-Paper.pdf) ‚¨Ö single intersection\n",
        "   5. [MPLight-AAAI-2020](https://ojs.aaai.org/index.php/AAAI/article/view/5744) ‚¨Ö Multi-intersection\n",
        "   6. [FM2Light-IROS-2024](https://ieeexplore.ieee.org/document/10801781) ‚¨ÖMARL\n",
        "   7. [UniLight-IJCAI-2022](https://www.ijcai.org/proceedings/2022/0535.pdf) ‚¨ÖMARL,Communication,Code Available\n",
        "\n"
      ],
      "metadata": {
        "id": "xQlEF37byy9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why MARL?**\n",
        "\n",
        "Due to the <U>exponentially growing action space</U> ü§Ø of reinforcement learning as the number of\n",
        "intersections increases, it becomes difficult to learn effective single-agent RL policies that can\n",
        "adapt to non-stationary environments like traffic signal control."
      ],
      "metadata": {
        "id": "4CnoWD3W3Bcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constrained Reinforcement Learning (CRL)**\n",
        "\n",
        "Constrained Reinforcement Learning (CRL) is an active research area in RL\n",
        "that solves such environments by developing algorithms that exclusively learn policies that are\n",
        "both effective and satisfy the constraints (e.g. safety, fairness, etc.)\n",
        "\n",
        "* [Constrained Policy Optimization (CPO) algorithm](https://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf)\n",
        "* [MACPO](https://arxiv.org/pdf/2110.02793) ‚¨Ö CPO into a multi-agent setting, [code](https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation) available\n",
        "* [C-MAA2C](https://proceedings.mlr.press/v211/tabas23a/tabas23a.pdf) ‚¨Ö Improve upon MACPO, [code](https://github.com/dtabas/multiagent-particle-envs) available"
      ],
      "metadata": {
        "id": "FJj5USaAj3wf"
      }
    }
  ]
}